{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-26T16:50:20.008376Z",
     "start_time": "2024-06-26T16:50:19.222851Z"
    }
   },
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import tempfile\n",
    "import json\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from openai import OpenAI\n",
    "import moviepy.editor as mp\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.schema import Document\n",
    "\n",
    "DATABASE_PATH = '/Users/mins/Desktop/github/bilibili_summarize/db/sqlite/bilibili.db'\n",
    "BASE_URL = '/Users/mins/Desktop/github/bilibili_summarize/static'\n",
    "COOKIE_PATH = '/Users/mins/Desktop/github/bilibili_summarize/cookie/cookie.json'\n",
    "\n",
    "class DyanmicTools:\n",
    "    def video_2_audio(self, id: str):\n",
    "        try:\n",
    "            videos_path = os.path.join(BASE_URL, 'video', f\"{id}.mp4\")\n",
    "            audio_path = os.path.join(BASE_URL, 'audio', f\"{id}.mp3\")\n",
    "\n",
    "            conn = sqlite3.connect(DATABASE_PATH)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM dynamic WHERE id = ?\", (id,))\n",
    "            exists = cursor.fetchone()[0]\n",
    "\n",
    "            my_clip = mp.VideoFileClip(videos_path)\n",
    "            my_clip.audio.write_audiofile(audio_path)\n",
    "\n",
    "            with open(audio_path, 'rb') as audio_file:\n",
    "                audio_bytes = audio_file.read()\n",
    "\n",
    "            if exists:\n",
    "                cursor.execute(\"UPDATE dynamic SET audio = ? WHERE id = ?\", (audio_bytes, id))\n",
    "            else:\n",
    "                cursor.execute(\"INSERT INTO dynamic (id, audio) VALUES (?, ?)\", (id, audio_bytes))\n",
    "\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "        except FileNotFoundError as e:\n",
    "            raise FileNotFoundError(f\"文件未找到: {e}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"发生错误: {e}\")\n",
    "\n",
    "    def audio_2_content(self, id: str):\n",
    "        if not id:\n",
    "            raise ValueError(\"id 未提供\")\n",
    "\n",
    "        client = OpenAI()\n",
    "\n",
    "        conn = sqlite3.connect(DATABASE_PATH)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT audio FROM dynamic WHERE id = ?\", (id,))\n",
    "        audio_data = cursor.fetchone()\n",
    "\n",
    "        if audio_data is None:\n",
    "            raise ValueError(\"未找到音频数据\")\n",
    "\n",
    "        temp_audio_file_path = None\n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as temp_audio_file:\n",
    "                temp_audio_file.write(audio_data[0])\n",
    "                temp_audio_file_path = temp_audio_file.name\n",
    "\n",
    "            with open(temp_audio_file_path, \"rb\") as audio_file:\n",
    "                transcript = client.audio.transcriptions.create(\n",
    "                    model=\"whisper-1\",\n",
    "                    file=audio_file,\n",
    "                    response_format=\"json\"\n",
    "                )\n",
    "            print(transcript)\n",
    "            content = transcript.text\n",
    "\n",
    "            cursor.execute(\"UPDATE dynamic SET content = ? WHERE id = ?\", (content, id))\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"OpenAI API 错误: {e}\")\n",
    "        finally:\n",
    "            if temp_audio_file_path:\n",
    "                os.remove(temp_audio_file_path)\n",
    "            conn.close()\n",
    "\n",
    "    def content_2_summary(self, id: str):\n",
    "        if not id:\n",
    "            raise ValueError(\"id 未提供\")\n",
    "\n",
    "        try:\n",
    "            prompt_template = \"\"\"\n",
    "            Write a concise summary of the following:\n",
    "            \"{text}\"\n",
    "            CONCISE SUMMARY，请用简体中文回答:\n",
    "            \"\"\"\n",
    "            prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "            llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "            llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "            stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
    "\n",
    "            conn = sqlite3.connect(DATABASE_PATH)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT content FROM dynamic WHERE id = ?\", (id,))\n",
    "            text = cursor.fetchone()\n",
    "\n",
    "            if text is None:\n",
    "                raise ValueError(\"未找到文本数据\")\n",
    "\n",
    "            documents = [Document(page_content=text[0])]\n",
    "            summary = stuff_chain.invoke(documents)[\"output_text\"]\n",
    "            print(summary)\n",
    "            cursor.execute(\"UPDATE dynamic SET summary = ? WHERE id = ?\", (summary, id))\n",
    "            conn.commit()\n",
    "        except FileNotFoundError as e:\n",
    "            raise FileNotFoundError(f\"File not found error: {e}\")\n",
    "        except IOError as e:\n",
    "            raise IOError(f\"IO error: {e}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"发生错误: {e}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def sent_bilibili_dynamic(self, id: str):\n",
    "        if not id:\n",
    "            raise ValueError(\"id 未提供\")\n",
    "\n",
    "        try:\n",
    "            with open(COOKIE_PATH, 'r') as file:\n",
    "                cookie = dict(json.load(file))\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"未查询到用户文件，请确认资源完整\")\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\"用户文件格式错误\")\n",
    "\n",
    "        ua = UserAgent()\n",
    "        url = \"https://api.vc.bilibili.com/dynamic_svr/v1/dynamic_svr/create\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "            \"User-Agent\": ua.random,\n",
    "            \"Cookie\": \"; \".join([f\"{key}={value}\" for key, value in cookie.items()])\n",
    "        }\n",
    "\n",
    "        conn = sqlite3.connect(DATABASE_PATH)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT summary FROM dynamic WHERE id = ?\", (id,))\n",
    "        summary = cursor.fetchone()[0]\n",
    "        print(summary)\n",
    "        content = f\"https://www.bilibili.com/video/{id}\\n{summary}\"\n",
    "        data = {\n",
    "            \"type\": 4,\n",
    "            \"rid\": 0,\n",
    "            \"content\": content\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, data=data)\n",
    "            response.raise_for_status()\n",
    "            cursor.execute(\"UPDATE dynamic SET is_sent = ? WHERE id = ?\", (1, id))\n",
    "            conn.commit()\n",
    "        except requests.RequestException as e:\n",
    "            raise Exception(f\"动态发布失败，错误信息: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"发生错误: {e}\")\n",
    "        finally:\n",
    "            conn.close()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T18:06:18.610150Z",
     "start_time": "2024-06-26T18:06:10.184474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/breakpoints/?h=workflow+add+node+agent#build-the-agent\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.prebuilt import ToolInvocation\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "processor = DyanmicTools()\n",
    "\n",
    "# class State(TypedDict):\n",
    "#     id: str\n",
    "#     video: str\n",
    "#     audio: str\n",
    "#     content: str\n",
    "#     summary: str\n",
    "#     dynamic: bool\n",
    "#     is_last_step: bool\n",
    "\n",
    "\n",
    "@tool\n",
    "def video_2_audio_tool(id: str):\n",
    "    \"\"\"\n",
    "    Convert a video to its corresponding audio by its bvid.\n",
    "\n",
    "    Args:\n",
    "        id (str): The ID of the video to convert to audio.\n",
    "\n",
    "    Returns:\n",
    "        str: The ID of the video that was successfully converted to audio.\n",
    "    \"\"\"\n",
    "    processor.video_2_audio(id)\n",
    "    return {\"id\": id}\n",
    "\n",
    "@tool\n",
    "def audio_2_content_tool(id: str):\n",
    "    \"\"\"\n",
    "    Convert a audio to its corresponding content by its bvid.\n",
    "\n",
    "    Args:\n",
    "        id (str): The ID of the audio to convert to content.\n",
    "\n",
    "    Returns:\n",
    "        str: The ID of the audio that was successfully converted to content.\n",
    "    \"\"\"\n",
    "    processor.audio_2_content(id)\n",
    "    return {\"id\": id}\n",
    "\n",
    "@tool\n",
    "def content_2_summary_tool(id: str):\n",
    "    \"\"\"\n",
    "    Summarize content by its ID.\n",
    "\n",
    "    Args:\n",
    "        id (str): The ID of the content to summarize.\n",
    "\n",
    "    Returns:\n",
    "        str: The summary of the content that was successfully generated.\n",
    "    \"\"\"\n",
    "    processor.content_2_summary(id)\n",
    "    return {\"id\": id}\n",
    "\n",
    "@tool\n",
    "def sent_bilibili_dynamic_tool(id: str):\n",
    "    \"\"\"\n",
    "    Summarize content by its ID and post the summary to Bilibili's dynamic.\n",
    "\n",
    "    Args:\n",
    "        id (str): The ID of the content to summarize.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the summary was successfully posted to Bilibili's dynamic.\n",
    "    \"\"\"\n",
    "    processor.sent_bilibili_dynamic(id)\n",
    "    return {\"id\": id}\n",
    "\n",
    "tools = [video_2_audio_tool, audio_2_content_tool, content_2_summary_tool, sent_bilibili_dynamic_tool]\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "model = ChatOpenAI(model = 'gpt-4o')\n",
    "model = model.bind_tools(tools)\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    print(\"last_message:\", last_message)\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the function to execute tools\n",
    "def call_tool(state):\n",
    "    messages = state[\"messages\"]\n",
    "    # Based on the continue condition\n",
    "    # we know the last message involves a function call\n",
    "    last_message = messages[-1]\n",
    "    # We construct an ToolInvocation from the function_call\n",
    "    tool_call = last_message.tool_calls[0]\n",
    "    action = ToolInvocation(\n",
    "        tool=tool_call[\"name\"],\n",
    "        tool_input=tool_call[\"args\"],\n",
    "    )\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    # We use the response to create a ToolMessage\n",
    "    tool_message = ToolMessage(\n",
    "        content=str(response), name=action.tool, tool_call_id=tool_call[\"id\"]\n",
    "    )\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [tool_message]}\n",
    "\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "\n",
    "from IPython.display import Image, display\n",
    "# 画出工作流图\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n",
    "\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "input = {\"messages\": [HumanMessage(content=\"请将视频（id：BV1AS421N7Rc）下载为音频，音频转为文本，再对文本进行摘要，并发送到bilibili动态\")]}\n",
    "\n",
    "# input = {\n",
    "#     \"id\":\"BV1AS421N7Rc\",\n",
    "#     \"video\": \"未完成\",\n",
    "#     \"audio\": \"未完成\",\n",
    "#     \"content\": \"未完成\",\n",
    "#     \"summary\": \"未完成\",\n",
    "#     \"dynamic\": False,\n",
    "# }\n",
    "# 执行工作流图，流式输出\n",
    "events = graph.stream(input)\n",
    "for s in events:\n",
    "    print(s)\n",
    "    print(\"----\")"
   ],
   "id": "66f136bd3ad0001b",
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_message: content='' additional_kwargs={'tool_calls': [{'id': 'call_bPZqwkD3dBYZf9TtLJBQVfPG', 'function': {'arguments': '{\"id\": \"BV1AS421N7Rc\"}', 'name': 'video_2_audio_tool'}, 'type': 'function'}, {'id': 'call_IYZdMAC93aBicP2dajiijqHl', 'function': {'arguments': '{\"id\": \"BV1AS421N7Rc\"}', 'name': 'audio_2_content_tool'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 304, 'total_tokens': 366}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_d576307f90', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-087d92a6-011d-4599-b10d-aaba7c7802df-0' tool_calls=[{'name': 'video_2_audio_tool', 'args': {'id': 'BV1AS421N7Rc'}, 'id': 'call_bPZqwkD3dBYZf9TtLJBQVfPG'}, {'name': 'audio_2_content_tool', 'args': {'id': 'BV1AS421N7Rc'}, 'id': 'call_IYZdMAC93aBicP2dajiijqHl'}] usage_metadata={'input_tokens': 304, 'output_tokens': 62, 'total_tokens': 366}\n",
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_bPZqwkD3dBYZf9TtLJBQVfPG', 'function': {'arguments': '{\"id\": \"BV1AS421N7Rc\"}', 'name': 'video_2_audio_tool'}, 'type': 'function'}, {'id': 'call_IYZdMAC93aBicP2dajiijqHl', 'function': {'arguments': '{\"id\": \"BV1AS421N7Rc\"}', 'name': 'audio_2_content_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 304, 'total_tokens': 366}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_d576307f90', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-087d92a6-011d-4599-b10d-aaba7c7802df-0', tool_calls=[{'name': 'video_2_audio_tool', 'args': {'id': 'BV1AS421N7Rc'}, 'id': 'call_bPZqwkD3dBYZf9TtLJBQVfPG'}, {'name': 'audio_2_content_tool', 'args': {'id': 'BV1AS421N7Rc'}, 'id': 'call_IYZdMAC93aBicP2dajiijqHl'}], usage_metadata={'input_tokens': 304, 'output_tokens': 62, 'total_tokens': 366})]}}\n",
      "----\n",
      "MoviePy - Writing audio in /Users/mins/Desktop/github/bilibili_summarize/static/audio/BV1AS421N7Rc.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "{'action': {'messages': [ToolMessage(content=\"{'id': 'BV1AS421N7Rc'}\", name='video_2_audio_tool', tool_call_id='call_bPZqwkD3dBYZf9TtLJBQVfPG')]}}\n",
      "----\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_IYZdMAC93aBicP2dajiijqHl\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mBadRequestError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 193\u001B[0m\n\u001B[1;32m    183\u001B[0m \u001B[38;5;66;03m# input = {\u001B[39;00m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;66;03m#     \"id\":\"BV1AS421N7Rc\",\u001B[39;00m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;66;03m#     \"video\": \"未完成\",\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    190\u001B[0m \u001B[38;5;66;03m# }\u001B[39;00m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;66;03m# 执行工作流图，流式输出\u001B[39;00m\n\u001B[1;32m    192\u001B[0m events \u001B[38;5;241m=\u001B[39m graph\u001B[38;5;241m.\u001B[39mstream(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m--> 193\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m events:\n\u001B[1;32m    194\u001B[0m     \u001B[38;5;28mprint\u001B[39m(s)\n\u001B[1;32m    195\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m----\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/langgraph/pregel/__init__.py:983\u001B[0m, in \u001B[0;36mPregel.stream\u001B[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001B[0m\n\u001B[1;32m    980\u001B[0m         \u001B[38;5;28;01mdel\u001B[39;00m fut, task\n\u001B[1;32m    982\u001B[0m \u001B[38;5;66;03m# panic on failure or timeout\u001B[39;00m\n\u001B[0;32m--> 983\u001B[0m _panic_or_proceed(done, inflight, step)\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# don't keep futures around in memory longer than needed\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m done, inflight, futures\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1537\u001B[0m, in \u001B[0;36m_panic_or_proceed\u001B[0;34m(done, inflight, step)\u001B[0m\n\u001B[1;32m   1535\u001B[0m             inflight\u001B[38;5;241m.\u001B[39mpop()\u001B[38;5;241m.\u001B[39mcancel()\n\u001B[1;32m   1536\u001B[0m         \u001B[38;5;66;03m# raise the exception\u001B[39;00m\n\u001B[0;32m-> 1537\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[1;32m   1539\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inflight:\n\u001B[1;32m   1540\u001B[0m     \u001B[38;5;66;03m# if we got here means we timed out\u001B[39;00m\n\u001B[1;32m   1541\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m inflight:\n\u001B[1;32m   1542\u001B[0m         \u001B[38;5;66;03m# cancel all pending tasks\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/concurrent/futures/thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs)\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/langgraph/pregel/retry.py:72\u001B[0m, in \u001B[0;36mrun_with_retry\u001B[0;34m(task, retry_policy)\u001B[0m\n\u001B[1;32m     70\u001B[0m task\u001B[38;5;241m.\u001B[39mwrites\u001B[38;5;241m.\u001B[39mclear()\n\u001B[1;32m     71\u001B[0m \u001B[38;5;66;03m# run the task\u001B[39;00m\n\u001B[0;32m---> 72\u001B[0m task\u001B[38;5;241m.\u001B[39mproc\u001B[38;5;241m.\u001B[39minvoke(task\u001B[38;5;241m.\u001B[39minput, task\u001B[38;5;241m.\u001B[39mconfig)\n\u001B[1;32m     73\u001B[0m \u001B[38;5;66;03m# if successful, end\u001B[39;00m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/langchain_core/runnables/base.py:2502\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   2498\u001B[0m config \u001B[38;5;241m=\u001B[39m patch_config(\n\u001B[1;32m   2499\u001B[0m     config, callbacks\u001B[38;5;241m=\u001B[39mrun_manager\u001B[38;5;241m.\u001B[39mget_child(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseq:step:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2500\u001B[0m )\n\u001B[1;32m   2501\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2502\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m step\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2503\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2504\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m step\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m, config)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/langgraph/utils.py:95\u001B[0m, in \u001B[0;36mRunnableCallable.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m accepts_config(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc):\n\u001B[1;32m     94\u001B[0m         kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m config\n\u001B[0;32m---> 95\u001B[0m     ret \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc, \u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ret, Runnable) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecurse:\n\u001B[1;32m     97\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ret\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m, config)\n",
      "Cell \u001B[0;32mIn[22], line 122\u001B[0m, in \u001B[0;36mcall_model\u001B[0;34m(state)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall_model\u001B[39m(state):\n\u001B[1;32m    121\u001B[0m     messages \u001B[38;5;241m=\u001B[39m state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m--> 122\u001B[0m     response \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39minvoke(messages)\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;66;03m# We return a list, because this will get added to the existing list\u001B[39;00m\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: [response]}\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/langchain_core/runnables/base.py:4573\u001B[0m, in \u001B[0;36mRunnableBindingBase.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   4567\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m   4568\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4569\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[1;32m   4570\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   4571\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[1;32m   4572\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n\u001B[0;32m-> 4573\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbound\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[1;32m   4574\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   4575\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_configs(config),\n\u001B[1;32m   4576\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs},\n\u001B[1;32m   4577\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:170\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    161\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    166\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[1;32m    167\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    168\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[1;32m    169\u001B[0m         ChatGeneration,\n\u001B[0;32m--> 170\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[1;32m    171\u001B[0m             [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[1;32m    172\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    173\u001B[0m             callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    174\u001B[0m             tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    175\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    176\u001B[0m             run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    177\u001B[0m             run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    178\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    179\u001B[0m         )\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    180\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:599\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    592\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    593\u001B[0m     prompts: List[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    596\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    597\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    598\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 599\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:456\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    454\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[1;32m    455\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 456\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    457\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    458\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[1;32m    459\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[1;32m    460\u001B[0m ]\n\u001B[1;32m    461\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:446\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[1;32m    444\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    445\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 446\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[1;32m    447\u001B[0m                 m,\n\u001B[1;32m    448\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    449\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    450\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    451\u001B[0m             )\n\u001B[1;32m    452\u001B[0m         )\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    454\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:671\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    670\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 671\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[1;32m    672\u001B[0m             messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    673\u001B[0m         )\n\u001B[1;32m    674\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    675\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:537\u001B[0m, in \u001B[0;36mBaseChatOpenAI._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    535\u001B[0m message_dicts, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_message_dicts(messages, stop)\n\u001B[1;32m    536\u001B[0m params \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[0;32m--> 537\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(messages\u001B[38;5;241m=\u001B[39mmessage_dicts, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[1;32m    538\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(response)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/openai/_utils/_utils.py:277\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 277\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/openai/resources/chat/completions.py:606\u001B[0m, in \u001B[0;36mCompletions.create\u001B[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    573\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    574\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    575\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    604\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[1;32m    605\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[0;32m--> 606\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n\u001B[1;32m    607\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/chat/completions\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    608\u001B[0m         body\u001B[38;5;241m=\u001B[39mmaybe_transform(\n\u001B[1;32m    609\u001B[0m             {\n\u001B[1;32m    610\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: messages,\n\u001B[1;32m    611\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: model,\n\u001B[1;32m    612\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrequency_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: frequency_penalty,\n\u001B[1;32m    613\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunction_call\u001B[39m\u001B[38;5;124m\"\u001B[39m: function_call,\n\u001B[1;32m    614\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunctions\u001B[39m\u001B[38;5;124m\"\u001B[39m: functions,\n\u001B[1;32m    615\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogit_bias\u001B[39m\u001B[38;5;124m\"\u001B[39m: logit_bias,\n\u001B[1;32m    616\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: logprobs,\n\u001B[1;32m    617\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_tokens,\n\u001B[1;32m    618\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\"\u001B[39m: n,\n\u001B[1;32m    619\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparallel_tool_calls\u001B[39m\u001B[38;5;124m\"\u001B[39m: parallel_tool_calls,\n\u001B[1;32m    620\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpresence_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: presence_penalty,\n\u001B[1;32m    621\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_format\u001B[39m\u001B[38;5;124m\"\u001B[39m: response_format,\n\u001B[1;32m    622\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseed\u001B[39m\u001B[38;5;124m\"\u001B[39m: seed,\n\u001B[1;32m    623\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m: stop,\n\u001B[1;32m    624\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream,\n\u001B[1;32m    625\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream_options\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream_options,\n\u001B[1;32m    626\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: temperature,\n\u001B[1;32m    627\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtool_choice\u001B[39m\u001B[38;5;124m\"\u001B[39m: tool_choice,\n\u001B[1;32m    628\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m\"\u001B[39m: tools,\n\u001B[1;32m    629\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_logprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_logprobs,\n\u001B[1;32m    630\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_p,\n\u001B[1;32m    631\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m: user,\n\u001B[1;32m    632\u001B[0m             },\n\u001B[1;32m    633\u001B[0m             completion_create_params\u001B[38;5;241m.\u001B[39mCompletionCreateParams,\n\u001B[1;32m    634\u001B[0m         ),\n\u001B[1;32m    635\u001B[0m         options\u001B[38;5;241m=\u001B[39mmake_request_options(\n\u001B[1;32m    636\u001B[0m             extra_headers\u001B[38;5;241m=\u001B[39mextra_headers, extra_query\u001B[38;5;241m=\u001B[39mextra_query, extra_body\u001B[38;5;241m=\u001B[39mextra_body, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[1;32m    637\u001B[0m         ),\n\u001B[1;32m    638\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mChatCompletion,\n\u001B[1;32m    639\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    640\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mStream[ChatCompletionChunk],\n\u001B[1;32m    641\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/openai/_base_client.py:1240\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1227\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1228\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1235\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1236\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1237\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1238\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1239\u001B[0m     )\n\u001B[0;32m-> 1240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls))\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/openai/_base_client.py:921\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    912\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[1;32m    913\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    914\u001B[0m     cast_to: Type[ResponseT],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    919\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    920\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m--> 921\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[1;32m    922\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m    923\u001B[0m         options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m    924\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    925\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m    926\u001B[0m         remaining_retries\u001B[38;5;241m=\u001B[39mremaining_retries,\n\u001B[1;32m    927\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/bilibili/lib/python3.11/site-packages/openai/_base_client.py:1020\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1017\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m   1019\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1020\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1022\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[1;32m   1023\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m   1024\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1027\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m   1028\u001B[0m )\n",
      "\u001B[0;31mBadRequestError\u001B[0m: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_IYZdMAC93aBicP2dajiijqHl\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [],
   "id": "45ab58dbfa41d3dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
